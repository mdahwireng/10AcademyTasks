{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d0cdd9",
   "metadata": {},
   "source": [
    "Load the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baef4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')     # This prevents displays of warnings which can be a distruction to viewing outputs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import STOPWORDS,WordCloud\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7247eb",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f134a",
   "metadata": {},
   "source": [
    "The data was downloaded from twitter and saved as a json file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98280b15",
   "metadata": {},
   "source": [
    "Creating a function to read and load the data form the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce43d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_file: str)->list:\n",
    "    \"\"\"\n",
    "    json file reader to open and read json files into a list\n",
    "    Args:\n",
    "    -----\n",
    "    json_file: str - path of a json file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    length of the json file and a list of json\n",
    "    \"\"\"\n",
    "    \n",
    "    tweets_data = []\n",
    "    for tweets in open(json_file,'r'):\n",
    "        tweets_data.append(json.loads(tweets))\n",
    "    \n",
    "    \n",
    "    return len(tweets_data), tweets_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27563362",
   "metadata": {},
   "source": [
    "Reading and loading the data with <b>read_json()</b> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e4df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = \"./data/covid19.json\"\n",
    "\n",
    "# reading the data and putting the total number of entries (tweet_len) and data (tweet_list) in variables\n",
    "tweet_len, tweet_list = read_json(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51827258",
   "metadata": {},
   "source": [
    "Creating a class and methods to extract the tweets and create a pandas dataframe from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b071a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDfExtractor:\n",
    "    \"\"\"\n",
    "    this function will parse tweets json into a pandas dataframe\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    dataframe\n",
    "    \"\"\"\n",
    "    def __init__(self, tweets_list):\n",
    "        \n",
    "        self.tweets_list = tweets_list\n",
    "\n",
    "    # an example function\n",
    "    def find_statuses_count(self)->list:\n",
    "        statuses_count = [i['user']['statuses_count'] for i in self.tweets_list]\n",
    "        return statuses_count \n",
    "        \n",
    "    def find_full_text(self)->list:\n",
    "        text = []\n",
    "        for i in self.tweets_list:\n",
    "            if 'retweeted_status' in i.keys():\n",
    "                if 'extended_tweet' in i['retweeted_status'].keys():\n",
    "                    text.append(i['retweeted_status']['extended_tweet']['full_text'])\n",
    "                else:\n",
    "                    text.append(i['text'])\n",
    "            else:\n",
    "                if 'extended_tweet' in i.keys():\n",
    "                    text.append(i['extended_tweet']['full_text'])\n",
    "                else:\n",
    "                    text.append(i['text'])\n",
    "        return text\n",
    "       \n",
    "    \n",
    "    def find_sentiments(self, text)->list:\n",
    "        polarity = [TextBlob(i).polarity for i in text]\n",
    "        self.subjectivity = [TextBlob(i).subjectivity for i in text]\n",
    "        return polarity, self.subjectivity\n",
    "\n",
    "    def find_created_time(self)->list:\n",
    "        created_at = [i['created_at'] for i in self.tweets_list]\n",
    "        return created_at\n",
    "\n",
    "    def find_source(self)->list:\n",
    "        source = [i['source'] for i in self.tweets_list]\n",
    "        return source\n",
    "\n",
    "    def find_screen_name(self)->list:\n",
    "        screen_name =[i['user']['screen_name'] for i in self.tweets_list]\n",
    "        return screen_name\n",
    "\n",
    "    def find_followers_count(self)->list:\n",
    "        followers_count = [i['user']['followers_count'] for i in self.tweets_list]\n",
    "        return followers_count\n",
    "\n",
    "    def find_friends_count(self)->list:\n",
    "        friends_count = [i['user']['friends_count'] for i in self.tweets_list]\n",
    "        return friends_count\n",
    "\n",
    "    def is_sensitive(self)->list:\n",
    "        is_sensitive = []\n",
    "        for i in self.tweets_list:\n",
    "            try:\n",
    "                is_sensitive.append(i['possibly_sensitive'])\n",
    "            except KeyError:\n",
    "                is_sensitive.append(None)\n",
    "        return is_sensitive\n",
    "\n",
    "    def find_favourite_count(self)->list:\n",
    "        favorite_count = []\n",
    "        for i in self.tweets_list:\n",
    "            if 'retweeted_status' in i.keys():\n",
    "                favorite_count.append(i['retweeted_status']['favorite_count'])\n",
    "            else:\n",
    "                favorite_count.append(i['favorite_count'])\n",
    "        return favorite_count\n",
    "    \n",
    "    def find_retweet_count(self)->list:\n",
    "        retweet_count = []\n",
    "        for i in self.tweets_list:\n",
    "            if 'retweeted_status' in i.keys():\n",
    "                retweet_count.append(i['retweeted_status']['retweet_count'])\n",
    "            else:\n",
    "                retweet_count.append(i['retweet_count'])\n",
    "        return retweet_count\n",
    "\n",
    "    def find_hashtags(self)->list:\n",
    "        hashtags = [i['entities']['hashtags'] for i in self.tweets_list]\n",
    "        return hashtags\n",
    "\n",
    "    def find_mentions(self)->list:\n",
    "        mentions = [i['entities']['user_mentions'] for i in self.tweets_list]\n",
    "        return mentions\n",
    "\n",
    "    def find_location(self)->list:\n",
    "        try:\n",
    "            location = [i['user']['location'] for i in self.tweets_list]\n",
    "        except TypeError:\n",
    "            location = ''\n",
    "        \n",
    "        return location\n",
    "\n",
    "    def find_lang(self)->list:\n",
    "        lang = [i['lang'] for i in self.tweets_list]\n",
    "        return lang\n",
    "    \n",
    "            \n",
    "    def get_tweet_df(self, save=False)->pd.DataFrame:\n",
    "        \n",
    "        columns = ['created_at', 'source', 'original_text','polarity','subjectivity', 'lang', 'favorite_count', 'retweet_count', \n",
    "            'original_author', 'followers_count','friends_count','possibly_sensitive', 'hashtags', 'user_mentions', 'place']\n",
    "        \n",
    "        created_at = self.find_created_time()\n",
    "        source = self.find_source()\n",
    "        text = self.find_full_text()\n",
    "        polarity, subjectivity = self.find_sentiments(text)\n",
    "        lang = self.find_lang()\n",
    "        fav_count = self.find_favourite_count()\n",
    "        retweet_count = self.find_retweet_count()\n",
    "        screen_name = self.find_screen_name()\n",
    "        follower_count = self.find_followers_count()\n",
    "        friends_count = self.find_friends_count()\n",
    "        sensitivity = self.is_sensitive()\n",
    "        hashtags = self.find_hashtags()\n",
    "        mentions = self.find_mentions()\n",
    "        location = self.find_location()\n",
    "        data = zip(created_at, source, text, polarity, subjectivity, lang, fav_count, retweet_count, screen_name, follower_count, friends_count, sensitivity, hashtags, mentions, location)\n",
    "        df = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "        if save:\n",
    "            df.to_csv('processed_tweet_data.csv', index=False)\n",
    "            print('File Successfully Saved.!!!')\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a69bb",
   "metadata": {},
   "source": [
    "Using the <b>TweetDfExtractor</b> class to create a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9221a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = TweetDfExtractor(tweet_list)    # creates an instance of TweetDfExtractor\n",
    "tweet_df = tweet.get_tweet_df()         # creates pandas dataframe using get_tweet_df method of TweetDfExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3b9e6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDfDataPreparation:\n",
    "    \"\"\"\n",
    "    this function will prepare tweets data form tweet dataframe modelling and visualization\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    dataframe\n",
    "    \"\"\"\n",
    "    def __init__(self, tweets_df):\n",
    "        \n",
    "        self.tweets_df = tweets_df\n",
    "        \n",
    "    def print_df_info(self) -> None:\n",
    "        \"\"\"\n",
    "        this function will print the info of the tweets datafame\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        None\n",
    "        \"\"\"\n",
    "        #save the number of columns and names\n",
    "        col_info = 'The number of colum(s): {}.\\nThe column(s) is/are : {} and {}'.format(len(self.tweets_df.columns),','.join(self.tweets_df.columns[:-2]), self.tweets_df.columns[-1])  \n",
    "        \n",
    "        #save the number of rows\n",
    "        num_rows = \"\\nThe total number of rows: {}\".format(len(self.tweets_df))\n",
    "        \n",
    "        #save the number of duplicate tweets\n",
    "        num_dup_tweets = '\\nThe number of duplicate tweets: {}'.format(len(self.tweets_df)-len(self.tweets_df.original_text.unique()))\n",
    "        \n",
    "        na_cols = self.tweets_df.columns[self.tweets_df.isnull().any()]\n",
    "        \n",
    "        #save the number of missing values\n",
    "        num_na_cols = \"\\nThe number of columns having missing value(s): {}\".format(len(na_cols))\n",
    "        \n",
    "        #save the columns with missing value and the num of values missing\n",
    "        na_cols_num_na = ''\n",
    "        \n",
    "        for col in na_cols:\n",
    "            na_cols_num_na += \"\\nThe number of rows with missing value(s) in [{}]: {}\".format(col, self.tweets_df[col].isnull().sum())\n",
    "        \n",
    "        # save the total number of missing values\n",
    "        tot_na = \"\\nThe total number of missing value(s): {}\".format(self.tweets_df.isnull().sum().sum())\n",
    "        \n",
    "        print(col_info, num_rows, num_dup_tweets, num_na_cols, na_cols_num_na, tot_na)\n",
    "        \n",
    "        \n",
    "    def slice_dataframe(self, columns=['created_at', 'original_text', 'polarity', 'subjectivity'],output=True)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        this function will slice of the tweets datafame. it takes a list of columns to slice and a bolean, output. \n",
    "        If its True it returns cleaned tweet\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        dataframe if output=True, None if output=False\n",
    "        \"\"\"\n",
    "        #sliced_tweet_df = self.tweets_df[columns]\n",
    "        self.sliced_tweet_df = self.tweets_df[columns]\n",
    "        if output:\n",
    "            return self.sliced_tweet_df\n",
    "        return None\n",
    "    \n",
    "    def drop_tweet_dup(self, column_name='original_text',output=True)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        this function will drop duplicates tweets in slicedtweet datafame. \n",
    "        it takes the name of column with the tweets in string format as an argument and \n",
    "        a bolean, output. If its True it returns cleaned tweet\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        dataframe if output=True, None if output=False\n",
    "        \"\"\"\n",
    "        sliced_tweet_df = self.sliced_tweet_df\n",
    "        sliced_tweet_df.drop_duplicates([column_name], inplace=True)\n",
    "        self.sliced_tweet_df = sliced_tweet_df\n",
    "        \n",
    "        if output:\n",
    "            return self.sliced_tweet_df\n",
    "        return None\n",
    "        \n",
    "        \n",
    "    def clean_tweet(self, column_name='original_text', cleaned_tweet_column_name='cleaned_tweet', output=True)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        this function will clean tweets in slicedtweet datafame. \n",
    "        it takes the name of column with the tweets and that of the new column for the cleaned tweet \n",
    "        both in string format as an argument and a bolean, output. If its True it returns cleaned tweet\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        dataframe if output=True, None if output=False\n",
    "        \"\"\"\n",
    "        unwanted = [\"'\\n',''\"]\n",
    "        take_out = ''\n",
    "        for char in unwanted:\n",
    "            take_out = string.punctuation + char \n",
    "        \n",
    "        def remove_punct_and_clean(tweet)->str:\n",
    "            # removes emojis\n",
    "            tweet = emoji.get_emoji_regexp().sub(r'', tweet)\n",
    "            # removes punctuations and newline characters\n",
    "            tweet  = \"\".join([char for char in tweet if char not in take_out])\n",
    "            # removes digits\n",
    "            tweet = re.sub('[0-9]+', '', tweet)\n",
    "            # converts to lowercase\n",
    "            tweet = tweet.lower()\n",
    "            return tweet\n",
    "        \n",
    "        sliced_tweet_df = self.sliced_tweet_df\n",
    "        sliced_tweet_df[cleaned_tweet_column_name] = sliced_tweet_df[column_name].apply(remove_punct_and_clean)\n",
    "        self.sliced_tweet_df = sliced_tweet_df\n",
    "        if output:\n",
    "            return self.sliced_tweet_df\n",
    "        return None\n",
    "        \n",
    "    def convert_to_datetime(self, column_name='created_at', output=True)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        this function will convert a parsed column in sliced tweet datafame to datetime. \n",
    "        it takes the name of column with the dates in string format as an argument and a bolean, output. \n",
    "        If its True it returns dataframe with the converted dates\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        dataframe if output=True, None if output=False\n",
    "        \"\"\"\n",
    "        sliced_tweet_df = self.sliced_tweet_df\n",
    "        sliced_tweet_df[column_name] = pd.to_datetime(sliced_tweet_df[column_name])\n",
    "        self.sliced_tweet_df = sliced_tweet_df\n",
    "\n",
    "        if output:\n",
    "            return self.sliced_tweet_df\n",
    "        return None\n",
    "    \n",
    "    def classify_polarity(self, column_name='polarity', cassified_column_name='classified_polarity', output=True)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        this function will classify a pared column in sliced tweet datafame with polarity. \n",
    "        it takes the name of column with the polarity and that of the new column for the classified polarityscores \n",
    "        both in string format as an argument and a bolean, output. If its True it returns a dataframe with the classified column added\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        dataframe if output=True, None if output=False\n",
    "        \"\"\"\n",
    "        \n",
    "        def classify(value)->str:\n",
    "            \"\"\"\n",
    "            this function will classify numbers. it takes the number be calssified as an argument\n",
    "            \n",
    "            Return\n",
    "            --------\n",
    "            string\n",
    "            \"\"\"\n",
    "            if value > 0.05:\n",
    "                return 'positive'\n",
    "            elif value < -0.05:\n",
    "                return 'negative'\n",
    "            else:\n",
    "                return 'neutral'\n",
    "            \n",
    "        sliced_tweet_df = self.sliced_tweet_df\n",
    "        sliced_tweet_df[cassified_column_name] = sliced_tweet_df[column_name].apply(classify)\n",
    "        self.sliced_tweet_df = sliced_tweet_df\n",
    "        if output:\n",
    "            return self.sliced_tweet_df\n",
    "        return None\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4cdf79c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_tweet(uncleaned_tweet_df)->pd.DataFrame:\n",
    "    prep = TweetDfDataPreparation(uncleaned_tweet_df)\n",
    "    prep.slice_dataframe(output=False)\n",
    "    prep.drop_tweet_dup(output=False)\n",
    "    prep.convert_to_datetime(output=False)\n",
    "    prep.classify_polarity(output=False)\n",
    "    cleaned = prep.clean_tweet()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def get_df_info(df)->None:\n",
    "    prep = TweetDfDataPreparation(df)\n",
    "    prep.print_df_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ff2dd649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of colum(s): 15.\n",
      "The column(s) is/are : created_at,source,original_text,polarity,subjectivity,lang,favorite_count,retweet_count,original_author,followers_count,friends_count,possibly_sensitive,hashtags and place \n",
      "The total number of rows: 6532 \n",
      "The number of duplicate tweets: 4237 \n",
      "The number of columns having missing value(s): 2 \n",
      "The number of rows with missing value(s) in [possibly_sensitive]: 5014\n",
      "The number of rows with missing value(s) in [place]: 2444 \n",
      "The total number of missing value(s): 7458\n"
     ]
    }
   ],
   "source": [
    "get_df_info(tweet_df)\n",
    "clened_tweet = get_cleaned_tweet(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f7d02c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2295.000000\n",
       "mean        0.434792\n",
       "std         0.285942\n",
       "min         0.000000\n",
       "25%         0.194765\n",
       "50%         0.480000\n",
       "75%         0.700000\n",
       "max         1.000000\n",
       "Name: subjectivity, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clened_tweet['subjectivity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abe9d7",
   "metadata": {},
   "source": [
    "Viewing the first 5 lines of the created dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcab9b9",
   "metadata": {},
   "source": [
    "Finding the information about the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['c'] = tweet_df['original_text'].apply(remove_punct_and_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d3aa6ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['🚨Africa is \"in the midst of a full-blown third wave\" of coronavirus, the head of @WHOAFRO has warned\\n\\nCases have risen across the continent by more than 20% and deaths have also risen by 15% in the last week\\n\\n@jriggers reports ~ 🧵\\nhttps://t.co/CRDhqPHFWM',\n",
       "       'Dr Moeti is head of WHO in Africa, and one of the best public health experts and leaders I know. Hers is a desperate request for vaccines to Africa. We plead with Germany and the UK to lift patent restrictions and urgently transfer technology to enable production in Africa. https://t.co/sOgIroihOc',\n",
       "       \"Thank you @research2note for creating this amazing campaign &amp; turning social media #red4research today. @NHSRDFORUM is all about sharing the talent, passion  &amp; commitment of individuals coming together as a community for the benefit of all. You've done this. Well done 👋\",\n",
       "       ...,\n",
       "       'I urge all the people of #India to take #Covid19 seriously and take your #vaccines as it’s the only way to beat this pandemic and get back to normalcy.  \\n#IndiaFightsCOVID19',\n",
       "       'Although there is not yet a vaccine for kids to help protect against COVID-19, make sure your child receives all recommended vaccines (including flu vaccine) appropriate for her age.\\n\\nCOVID Prevention Tips by Dr Tejas Limaye , Team JFH \\n\\n#COVID19\\n#HealThyLife\\n#JustForHearts https://t.co/3EkDxegFBn',\n",
       "       '@Jenfeds73 @DcrInYYC Respectfully, veterinarians have to have a broad knowledge of a multitude of viruses, including coronaviruses, and also help develop vaccines that combat zoonotic viruses, COVID19 being one of them. Without vets, we likely wouldn’t have had this vaccine as quickly.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['original_text'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc92c9",
   "metadata": {},
   "source": [
    "Displaying the information about null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3062b",
   "metadata": {},
   "source": [
    "From the unformation displayed about the data, it was seen that there are 6532 entries in all and 2 columns out of the 15 with missing values. The number of missing values in these rows as compared to the total number of entries suggest that those columns will not have significant number of entries for analysis. Therefore the 2 columns were droped.\n",
    "\n",
    "The chosen columns were created_at, original_text, hashtags and user_mentions. The original_text was be cleaned to create the cleaned_text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719fc099",
   "metadata": {},
   "source": [
    "### Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e090c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of data for analysis\n",
    "selected_df = tweet_df[['created_at', 'original_text', 'hashtags', 'user_mentions','place']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f964f2f",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48032c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique entries of \"tweets\" (original_text column) out of {} entries'\n",
    "      .format(len(selected_df.original_text.unique()), tweet_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf0190",
   "metadata": {},
   "source": [
    "The information derived from the unique entries of tweet indicates that there are duplicate values. \n",
    "These duplicates values were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes duplicates of tweets\n",
    "selected_df['original_text'] = selected_df.original_text.drop_duplicates()\n",
    "\n",
    "# Drops missing values\n",
    "selected_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b01d1b",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a string of characters to be taken out\n",
    "unwanted = [\"'\\n'\",\"’\"]\n",
    "take_out = ''\n",
    "for char in unwanted:\n",
    "    take_out = string.punctuation + char "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean tweet \n",
    "\n",
    "def remove_punct_and_clean(tweet):\n",
    "    \n",
    "    for char in unwanted:\n",
    "        take_out = string.punctuation + char \n",
    "    tweet = emoji.get_emoji_regexp().sub(r'', tweet)\n",
    "    tweet  = \"\".join([char for char in tweet if char not in take_out])\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f783f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans and adds column to selected_df\n",
    "selected_df['cleaned_text'] = selected_df['original_text'].apply(remove_punct_and_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts to string\n",
    "selected_df['cleaned_text'] = selected_df['cleaned_text'].astype(str)                   \n",
    "\n",
    "# converts to string\n",
    "selected_df['cleaned_text'] = selected_df['cleaned_text'].apply(lambda x: x.lower())    # converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ba3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays a figure of the most used words\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(WordCloud(width=1000,height=600,stopwords=STOPWORDS).generate(' '.join(selected_df.cleaned_text.values)))\n",
    "plt.axis('off')\n",
    "plt.title('Most Frequent Words In Our Tweets',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82dc04a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gm = {hi:9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e152d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm['hi']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
